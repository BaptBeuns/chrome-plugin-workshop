{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Privacy Text Classification \n",
    "If a provider wants to sell data on the data store, the provider would have to tag the data and it would go into a queue. The PO in change would take a look at the segments and label the segments with one of the following classifications: GA, R1, R2. GA refers to general data, R1 refers to some more private data (e.g. income) and R2 refers to even more private data (e.g. health). The PO can refer to the privacy team for assistance. This process can slow down the provider from releasing the data. \n",
    "(See https://docs.google.com/document/d/14ltFfoN0bRWdT71Zaw5A60poGZjG60XHjFSdHhuGfXM/edit?ts=58798cc6# for more information)\n",
    "\n",
    "\n",
    "With the use of natural language processing, we hope to streamline this process by having a machine classify the data and limit the amount of human intervention necessary.\n",
    "\n",
    "There are many methods of text classification. One prominent method is to use word2vec to convert the words into vectors and run classical algorithms on it (more success has been seen in word-based ConvNets and LSTMs on the vectors). Traditional NLP methods use bag-of-words, which would involve picking out the most frequent words in the entire data set and counting how often each word appears in the sample. \n",
    "\n",
    "\n",
    "Why we decided not to use bag of words... actually we can try bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import sklearn\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "The data used in this project were obtained from the following link: https://docs.google.com/spreadsheets/d/1ySI6QxVbsy6fLqlIn2-T0awo82whu87scO6RTpvthUY/edit#gid=463244196\n",
    "\n",
    "Currently, we pick 2000 random GA segment names because the size of our GA far exceeds what we have in R1 and R2. This is to prevent class imbalance. This is a temporary workaround. Not sure how to approach this yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GA\n",
    "ga_list = []\n",
    "with open(\"GA.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ga_list.append(row['name'])\n",
    "random.shuffle(ga_list)\n",
    "ga_list = ga_list[:2000] # GA has too many data points \n",
    "\n",
    "# Set up r1\n",
    "r1_list = []\n",
    "with open(\"R1.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        r1_list.append(row['name'])\n",
    "        \n",
    "# Set up r2\n",
    "r2_list = []\n",
    "with open(\"R2.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        r2_list.append(row['name'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for word2vec\n",
    "word2vec is the main tool used to convert words to vectors in a sensible fashion. We use the pretrained word2vec model provided by Google.\n",
    "\n",
    "TODO: How we combine the vectors is slightly hackish and needs further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-330-e2713766472d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Google's pre-trained Word2Vec model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jeffzh/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeffzh/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocabulary file is incomplete: '%s' is missing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vectors(lst):\n",
    "    # given a list of strings\n",
    "    # use google's word2vec to convert words to their vector forms\n",
    "    # found average of the vectors in a name\n",
    "    # return a list of vectors and dictionary from input string to its vector \n",
    "    word_score_pairs = {}\n",
    "    ret = []\n",
    "    for entry in lst:\n",
    "        word_list = []\n",
    "        for words in entry.strip().split(\" > \"):\n",
    "            word_list.extend(words.strip().split(\" \"))\n",
    "        sum_vect = np.zeros(300)\n",
    "        num_success = 0\n",
    "        for word in word_list:\n",
    "            try: \n",
    "                sum_vect += model[word.strip()]\n",
    "                num_success += 1\n",
    "            except KeyError: \n",
    "                continue\n",
    "        entry_vect = 1.0*sum_vect/max(num_success, 1)\n",
    "        word_score_pairs[entry.strip()] = entry_vect\n",
    "        ret.append(entry_vect)\n",
    "    return ret, word_score_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, r1_pairs = calculate_vectors(r1_list)\n",
    "r2, r2_pairs = calculate_vectors(r2_list)\n",
    "ga, ga_pairs = calculate_vectors(ga_list)\n",
    "word_score_pairs = {**r1_pairs, **r2_pairs, **ga_pairs} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look into the vectors and data\n",
    "Mainly used to justify that word2vec has provided sensible word to vector mapping and the data is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff(x,y):\n",
    "    # Calculate euclidean distance between two points\n",
    "    print(np.linalg.norm(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some viability to the vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'V12 > PYCO PERSONALITY > Mature Social Media Users'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-373-09945fb8c70a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Crossix US > Healthcare > Prescription Type > Depression\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Crossix US > Healthcare > Conversion > Uloric - Packaged\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mv4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"V12 > PYCO PERSONALITY > Mature Social Media Users\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'V12 > PYCO PERSONALITY > Mature Social Media Users'"
     ]
    }
   ],
   "source": [
    "print(\"Some viability to the vectors\")\n",
    "w = word_score_pairs\n",
    "v1 = w[\"Crossix US > Healthcare > Prescription Type > Insomnia\"]\n",
    "v2 = w[\"Crossix US > Healthcare > Prescription Type > Depression\"]\n",
    "v3 = w[\"Crossix US > Healthcare > Conversion > Uloric - Packaged\"]\n",
    "v4 = w[\"V12 > PYCO PERSONALITY > Mature Social Media Users\"]\n",
    "diff(v1,v2)\n",
    "diff(v1,v3)\n",
    "diff(v2,v3)\n",
    "diff(v1,v4)\n",
    "diff(v2,v4)\n",
    "diff(v3,v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_words(word_score_pairs):\n",
    "    # input is dictionary with keys as the segment_names\n",
    "    # return all the words used in the segment_names \n",
    "    words = []\n",
    "    word_list = []\n",
    "    for words_with_greater in word_score_pairs.keys():\n",
    "        word_list.extend(words_with_greater.split(' > '))\n",
    "    for words_with_spaces in word_list:\n",
    "        words.extend(words_with_spaces.split(' '))\n",
    "    words = [word for word in words if word != '-']\n",
    "    return words\n",
    "words = get_all_words(word_score_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 47575\n",
      "Total number of unique words: 6758\n",
      "Total number of unique words found by Google's word2vec model: 4036\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words: \" + str(len(words)))\n",
    "print(\"Total number of unique words: \" + str(len(set(words))))\n",
    "num_found = 0\n",
    "for word in set(words): \n",
    "    try:\n",
    "        model[word]\n",
    "        num_found += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "print(\"Total number of unique words found by Google's word2vec model: \" + str(num_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "We want to create a combined X and y variables and then split the data such that we have a training set, validation set, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_and_label_data(data_segments):\n",
    "    # labels data. pass in an list of lists of data of a partciular segment\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_segments)):\n",
    "        X += data_segments[i]\n",
    "        y += [i] * len(data_segments[i])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X,y):\n",
    "    # Uses sklearn to split data \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each class in training set: [1421, 1475, 1495]\n",
      "Count of each class in validation set: [296, 323, 323]\n",
      "Count of each class in test set: [283, 339, 319]\n"
     ]
    }
   ],
   "source": [
    "X, y = merge_and_label_data([ga, r1, r1])\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(X,y)\n",
    "print(\"Count of each class in training set: \" + str([y_train.count(i) for i in range(3)]))\n",
    "print(\"Count of each class in validation set: \" + str([y_val.count(i) for i in range(3)]))\n",
    "print(\"Count of each class in test set: \" + str([y_test.count(i) for i in range(3)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers \n",
    "Use x_train, y_train to train your models. \n",
    "\n",
    "Use x_val, y_val to tune your hyperparameters.\n",
    "\n",
    "Use x_test, y_test to test your accuracy. DO NOT USE TEST DATA UNTIL YOU HAVE FOUND DESIRED HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor Classifier\n",
    "The classic technique to provide a baseline accuracy for other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40594059,  0.27722772,  0.31683168],\n",
       "       [ 0.33663366,  0.35643564,  0.30693069]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=101)\n",
    "knn.fit(x_train, y_train)\n",
    "knn.predict_proba([model[\"Healthcare\"], r2[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.602972399151\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(knn.predict(x_val), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 3  => accuracy = 0.59130\n",
    "# k = 5  => accuracy = 0.58386\n",
    "# k = 10  => accuracy = 0.5637\n",
    "# k = 15  => accuracy = 0.583\n",
    "# k = 100  => accuracy = 0.576"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radius Neighbor Classifier \n",
    "Task for Shida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543524416136\n"
     ]
    }
   ],
   "source": [
    "# playing around with logistic regression \n",
    "log_reg = LogisticRegression(C=1)\n",
    "log_reg.fit(x_train, y_train)\n",
    "print(accuracy_score(log_reg.predict(x_val), y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE BELOW FOR NOW\n",
    "## Bag of Words\n",
    "Another way to generate the vectors to use in classical machine learning models is to use bag of words.\n",
    "We pick the words that show up the most frequently in our data set and for each sample, determine the number of times certain words show up in the sample. These will be the features. \n",
    "\n",
    "We have to pick the number of words to look out for. The following paper used the 50,000 most frequent words: https://arxiv.org/abs/1509.01626 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# NEEDS TO HAVE RUN calculate_vector (even though different representation) \n",
    "def get_frequency_map(words): \n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        if word not in freq.keys():\n",
    "            freq[word] = 1\n",
    "        else: \n",
    "            freq[word] +=1\n",
    "    return freq\n",
    "\n",
    "def get_sorted_freq_list(words):\n",
    "    freq = get_frequency_map(words)\n",
    "    sorted_freq_list = sorted(freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_freq_list\n",
    "\n",
    "def pick_bag_of_words_words(words, num_words=100):\n",
    "    sorted_freq_list = get_sorted_freq_list(words)[:num_words]\n",
    "    zipped_list = zip(*sorted_freq_list)\n",
    "    return list(list(zipped_list)[0])\n",
    "\n",
    "def calculate_bag_of_words_vector(lst, bow_words):\n",
    "    # given a list of strings\n",
    "    # use bag_of_words to convert words to their vector forms\n",
    "    # return a list of vectors \n",
    "    # WARNING this method can be really slow. Look to refactor it \n",
    "    ret = []\n",
    "    vector_length = len(bow_words)\n",
    "    for entry in lst:\n",
    "        bow_vector = [0] * vector_length\n",
    "        for words in entry.strip().split(\" > \"):\n",
    "            for word in words.split(\" \"):\n",
    "                word = word.strip()\n",
    "                try: \n",
    "                    bow_vector[bow_words.index(word)] +=1\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        ret.append(bow_vector)\n",
    "    return ret\n",
    "\n",
    "# Example use case\n",
    "bow_words = pick_bag_of_words_words(words, 500)\n",
    "r1_bow = calculate_bag_of_words_vector(r1_list, bow_words)\n",
    "print(sum(r1_bow[441]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
